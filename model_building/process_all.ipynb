{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import umap\n",
    "from tqdm.autonotebook import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import language\n",
    "import text_nn\n",
    "import grab_category\n",
    "import news\n",
    "import groups\n",
    "import libs.cpp_stuff as cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = [os.path.join(\"data\", folder) for folder in [\"sample\", \"sample2\", \"sample3\", \"sample4\", \"sample5\", \"website/en\", \"website/ru\", \"sample6\", \"sample7\"]]\n",
    "train_folders = [os.path.join(\"data\", folder) for folder in [\"sample\", \"sample2\", \"sample3\", \"sample4\", \"sample5\", \"sample6\", \"sample7\"]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for folder in all_folders:\n",
    "    for sub in [\"langs\", \"categories_ru\"]:\n",
    "        if os.path.exists(os.path.join(folder, sub)):\n",
    "            shutil.rmtree(os.path.join(folder, sub))\n",
    "        \n",
    "    for file in [\"all\"]:\n",
    "        if os.path.exists(os.path.join(folder, file)):\n",
    "            os.remove(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sample\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f741a98a07847469dc1ffee74556f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/sample2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07ffc950b5e44c79d9e9c5e6bba49c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/sample3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6d790f7cad4d91ba0c8022b6491317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/sample4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b051ccfd9407a9d4cbf1b7d93f9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data/sample5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b572d9a5b14529bb27c485b9c9f21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs = Counter()\n",
    "\n",
    "for folder in all_folders:\n",
    "    print(folder)\n",
    "    for day_folder in tqdm(glob.glob(os.path.join(folder, \"????????\"))):\n",
    "        for batch_folder in glob.glob(os.path.join(day_folder, \"??\")):\n",
    "            htmls = glob.glob(os.path.join(batch_folder, \"*.html\"))\n",
    "            for html in htmls:\n",
    "                file_data = language.read_file(html, [])\n",
    "                paragraphs.update(file_data[\"paragraphs\"])\n",
    "\n",
    "ignore_limit = 50\n",
    "with open(\"data/ignore_paragraph\", \"w\") as f:\n",
    "    for p, c in paragraphs.most_common():\n",
    "        if c < ignore_limit:\n",
    "            break\n",
    "            \n",
    "        f.write(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de57d0b71da14bee910c628851ff4a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(language.site_languages_file):\n",
    "    site_languages = {}\n",
    "    for folder in tqdm(all_folders):\n",
    "        language.collect_site_languages(folder, site_languages)\n",
    "        \n",
    "    for site, ctr in site_languages.items():\n",
    "        total = sum(ctr.values())\n",
    "        site_languages[site] = {code: count / total for code, count in ctr.items()}\n",
    "\n",
    "    with open(language.site_languages_file, \"w\") as f:\n",
    "        json.dump(site_languages, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(language.site_languages_file) as f:\n",
    "    site_languages = json.load(f)\n",
    "    cpp.load_sources_languages(site_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c6ecabe78645a885c3f01d379b4543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sample\n",
      "Skipping\n",
      "data/sample2\n",
      "Skipping\n",
      "data/sample3\n",
      "Skipping\n",
      "data/sample4\n",
      "Skipping\n",
      "data/sample5\n",
      "Skipping\n",
      "data/website/en\n",
      "Skipping\n",
      "data/website/ru\n",
      "Skipping\n",
      "data/sample6\n",
      "Skipping\n",
      "data/sample7\n"
     ]
    }
   ],
   "source": [
    "for folder in tqdm(all_folders):\n",
    "    print(folder)\n",
    "    all_file = language.merge_folder(folder)\n",
    "    if os.path.exists(os.path.join(folder, \"langs\")):\n",
    "        print(\"Skipping\")\n",
    "        continue\n",
    "        shutil.rmtree(os.path.join(folder, \"langs\"))\n",
    "    \n",
    "    file_info = language.read_dump(all_file)\n",
    "    lang_data = language.detect_languages(file_info)\n",
    "    lang_dumps = os.path.join(folder, \"langs\")\n",
    "    os.makedirs(lang_dumps)\n",
    "    for ld in lang_data:\n",
    "        language.dump_texts(ld[\"articles\"], os.path.join(lang_dumps, ld[\"lang_code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410 lenta.ru files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7ad333752c4a3bbad4ea3423b08ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=119414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1106 GT labels saved to data/sample6\n"
     ]
    }
   ],
   "source": [
    "for folder in all_folders:\n",
    "    if os.path.exists(os.path.join(folder, \"ground_truth\")) or \"website\" in folder:\n",
    "        continue\n",
    "        \n",
    "    file_info = language.read_dump(os.path.join(folder, \"langs\", \"ru\"))\n",
    "    gts = [grab_category.grab_lenta_ru_categories(file_info), grab_category.grab_nv_categories(file_info), grab_category.grab_federalpress_categories(file_info),\n",
    "           grab_category.grab_korrnet_categories(file_info), grab_category.grab_allhockey_categories(file_info)]\n",
    "    \n",
    "    gt = grab_category.join_gt(gts)\n",
    "    grab_category.save_gt(gt, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106 GT labels loaded from data/sample6\n",
      "7410 GT labels saved to data/sample6\n"
     ]
    }
   ],
   "source": [
    "for folder in all_folders[-1:]:\n",
    "    file_info = language.read_dump(os.path.join(folder, \"langs\", \"en\"))\n",
    "    gts = [grab_category.load_gt(folder), grab_category.grab_reuters_categories(file_info), grab_category.grab_theguardian_categories(file_info), \n",
    "           grab_category.grab_mirror_categories(file_info)]\n",
    "    \n",
    "    gt = grab_category.join_gt(gts)\n",
    "    grab_category.save_gt(gt, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sample\n",
      "data/sample2\n",
      "data/sample3\n",
      "data/sample4\n",
      "data/sample5\n",
      "data/website/en\n",
      "data/website/ru\n",
      "data/sample6\n"
     ]
    }
   ],
   "source": [
    "for folder in tqdm(all_folders):\n",
    "    print(folder)\n",
    "    if os.path.exists(os.path.join(folder, \"categories_ru\")):\n",
    "        continue\n",
    "        shutil.rmtree(os.path.join(folder, \"categories_ru\"))\n",
    "\n",
    "    ru_dump = os.path.join(folder, \"langs\", \"ru\")\n",
    "    if not os.path.exists(ru_dump):\n",
    "        continue\n",
    "        \n",
    "    file_info = language.read_dump(ru_dump)\n",
    "    ru_categories = news.classify_news(file_info)\n",
    "\n",
    "    os.makedirs(os.path.join(folder, \"categories_ru\"))\n",
    "    for cd in ru_categories:\n",
    "        language.dump_texts(cd[\"articles\"], os.path.join(folder, \"categories_ru\", cd[\"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a52bdfc1d949b4aa83c8a994189b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sample\n",
      "data/sample2\n",
      "data/sample3\n",
      "data/sample4\n",
      "data/sample5\n",
      "data/website/en\n",
      "data/website/ru\n",
      "data/sample6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for folder in tqdm(all_folders):\n",
    "    print(folder)\n",
    "    if os.path.exists(os.path.join(folder, \"categories_en\")):\n",
    "        continue\n",
    "        shutil.rmtree(os.path.join(folder, \"categories_en\"))\n",
    "\n",
    "    en_dump = os.path.join(folder, \"langs\", \"en\")\n",
    "    if not os.path.exists(en_dump):\n",
    "        continue\n",
    "        \n",
    "    file_info = language.read_dump(en_dump)\n",
    "    en_categories = news.classify_news(file_info)\n",
    "\n",
    "    os.makedirs(os.path.join(folder, \"categories_en\"))\n",
    "    for cd in en_categories:\n",
    "        language.dump_texts(cd[\"articles\"], os.path.join(folder, \"categories_en\", cd[\"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru: 423873 examples\n",
      "en: 394408 examples\n"
     ]
    }
   ],
   "source": [
    "for lang in [\"ru\", \"en\"]:\n",
    "    texts_for_grouping = []\n",
    "    for folder in tqdm(train_folders):\n",
    "        dump_file = os.path.join(folder, \"langs\", lang)\n",
    "        if os.path.exists(dump_file):\n",
    "            texts_for_grouping.extend(groups.extract_texts_for_grouping(language.read_dump(dump_file)))\n",
    "\n",
    "    print(f\"{lang}: {len(texts_for_grouping)} examples\")\n",
    "    cpp.make_idf(texts_for_grouping, f\"data/chunk_counts_{lang}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"society\", \"economy\", \"sports\", \"science\", \"other\", \"technology\", \"entertainment\"]\n",
    "\n",
    "for folder in all_folders:\n",
    "    print(folder)\n",
    "    if os.path.exists(os.path.join(folder, \"threads_ru\")):\n",
    "        os.remove(os.path.join(folder, \"threads_ru\"))\n",
    "    \n",
    "    file_info = []\n",
    "    cats = []\n",
    "    similarities = []\n",
    "    for cat in categories:\n",
    "        cat_fi = language.read_dump(os.path.join(folder, \"categories_ru\", cat))\n",
    "        \n",
    "        texts = groups.extract_text_for_grouping(cat_fi)\n",
    "        process_input = \"\\n\".join([f\"{len(texts)}\"] + texts) + \"\\n\"\n",
    "        output = language.run_process(\n",
    "            [\"groups/Release/news_groups\", \"similarity\", groups.counts_for_grouping_ru], process_input)\n",
    "        \n",
    "        cat_sims = [[y.split(\" \") for y in x.strip().split(\"\\t\") if y != \"\"] for x in output[:-1].split(\"\\n\")]           \n",
    "        cat_sims = [[(int(idx) + len(file_info), float(sim)) for idx, sim in sims] for sims in cat_sims]\n",
    "        similarities.extend(cat_sims)\n",
    "        file_info.extend(cat_fi)\n",
    "        cats.extend([cat] * len(cat_fi))        \n",
    "        \n",
    "    with open(os.path.join(folder, \"threads_ru\"), \"wb\") as f:\n",
    "        pickle.dump((file_info, cats, similarities), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sample\n",
      "data/sample2\n",
      "data/sample3\n",
      "data/sample4\n"
     ]
    }
   ],
   "source": [
    "categories = [\"society\", \"economy\", \"sports\", \"science\", \"other\", \"technology\", \"entertainment\"]\n",
    "\n",
    "for folder in all_folders:\n",
    "    print(folder)\n",
    "    if os.path.exists(os.path.join(folder, \"threads_en\")):\n",
    "        os.remove(os.path.join(folder, \"threads_en\"))\n",
    "    \n",
    "    file_info = []\n",
    "    cats = []\n",
    "    similarities = []\n",
    "    for cat in categories:\n",
    "        cat_fi = language.read_dump(os.path.join(folder, \"categories_en\", \"dump\", cat))\n",
    "        \n",
    "        texts = groups.extract_text_for_grouping(cat_fi)\n",
    "        process_input = \"\\n\".join([f\"{len(texts)}\"] + texts) + \"\\n\"\n",
    "        output = language.run_process(\n",
    "            [\"groups/Release/news_groups\", \"similarity\", groups.counts_for_grouping_en], process_input)\n",
    "        \n",
    "        cat_sims = [[y.split(\" \") for y in x.strip().split(\"\\t\") if y != \"\"] for x in output[:-1].split(\"\\n\")]           \n",
    "        cat_sims = [[(int(idx) + len(file_info), float(sim)) for idx, sim in sims] for sims in cat_sims]\n",
    "        similarities.extend(cat_sims)\n",
    "        file_info.extend(cat_fi)\n",
    "        cats.extend([cat] * len(cat_fi))\n",
    "        \n",
    "        \n",
    "    with open(os.path.join(folder, \"threads_en\"), \"wb\") as f:\n",
    "        pickle.dump((file_info, cats, similarities), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
