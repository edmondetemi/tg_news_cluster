{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have TensorFlow version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import umap\n",
    "from tqdm.autonotebook import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "models = keras.models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# This code was tested with TensorFlow v1.8\n",
    "print(\"You have TensorFlow version\", tf.__version__)\n",
    "\n",
    "import language\n",
    "import news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            words = line.split(\" \")\n",
    "            label = words[0].replace(\"__label__\", \"\")\n",
    "            text = \" \".join(words[1:])\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "\n",
    "    data = pd.DataFrame({\"category\": labels, \"text\": texts})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"data/category_train.txt\")\n",
    "test_data = load_data(\"data/category_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "society          628\n",
      "junk             232\n",
      "economy          201\n",
      "other            138\n",
      "sports            78\n",
      "science           44\n",
      "technology        38\n",
      "entertainment     31\n",
      "Name: category, dtype: int64\n",
      "society          13408\n",
      "sports            3296\n",
      "other             2436\n",
      "economy           2274\n",
      "entertainment     1983\n",
      "junk              1279\n",
      "science           1034\n",
      "technology         618\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['category'].value_counts())\n",
    "print(test_data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, \n",
    "                                              char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_data.text) # fit tokenizer to our training text data\n",
    "x_train = tokenize.texts_to_matrix(train_data.text)\n",
    "x_test = tokenize.texts_to_matrix(test_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data.category)\n",
    "y_train_num = encoder.transform(train_data.category)\n",
    "y_test_num = encoder.transform(test_data.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "num_classes = np.max(y_train_num) + 1\n",
    "y_train = keras.utils.to_categorical(y_train_num, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test_num, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1390, 10000)\n",
      "x_test shape: (26328, 10000)\n",
      "y_train shape: (1390, 8)\n",
      "y_test shape: (26328, 8)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model trains very quickly and 2 epochs are already more than enough\n",
    "# Training for more epochs will likely lead to overfitting on this dataset\n",
    "# You can try tweaking these hyperparamaters when using this model with your own data\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "drop_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(512, input_shape=(max_words,), use_bias=True))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(drop_ratio))\n",
    "model.add(layers.Dense(512, use_bias=True))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(drop_ratio))\n",
    "model.add(layers.Dense(num_classes))\n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1251 samples, validate on 139 samples\n",
      "Epoch 1/20\n",
      "1251/1251 [==============================] - 2s 1ms/sample - loss: 2.4561 - accuracy: 0.1663 - val_loss: 2.0183 - val_accuracy: 0.3381\n",
      "Epoch 2/20\n",
      "1251/1251 [==============================] - 0s 379us/sample - loss: 1.9350 - accuracy: 0.2918 - val_loss: 1.9053 - val_accuracy: 0.4604\n",
      "Epoch 3/20\n",
      "1251/1251 [==============================] - 0s 371us/sample - loss: 1.5900 - accuracy: 0.4349 - val_loss: 1.7599 - val_accuracy: 0.5180\n",
      "Epoch 4/20\n",
      "1251/1251 [==============================] - 0s 388us/sample - loss: 1.3583 - accuracy: 0.5316 - val_loss: 1.5980 - val_accuracy: 0.6043\n",
      "Epoch 5/20\n",
      "1251/1251 [==============================] - 0s 386us/sample - loss: 1.2034 - accuracy: 0.5891 - val_loss: 1.4518 - val_accuracy: 0.6187\n",
      "Epoch 6/20\n",
      "1251/1251 [==============================] - 0s 359us/sample - loss: 1.0843 - accuracy: 0.6339 - val_loss: 1.2938 - val_accuracy: 0.6691\n",
      "Epoch 7/20\n",
      "1251/1251 [==============================] - 0s 357us/sample - loss: 0.9483 - accuracy: 0.6787 - val_loss: 1.1437 - val_accuracy: 0.6906\n",
      "Epoch 8/20\n",
      "1251/1251 [==============================] - 0s 381us/sample - loss: 0.8415 - accuracy: 0.7170 - val_loss: 1.0270 - val_accuracy: 0.7266\n",
      "Epoch 9/20\n",
      "1251/1251 [==============================] - 0s 375us/sample - loss: 0.7794 - accuracy: 0.7450 - val_loss: 0.9291 - val_accuracy: 0.7338\n",
      "Epoch 10/20\n",
      "1251/1251 [==============================] - 0s 366us/sample - loss: 0.6799 - accuracy: 0.7954 - val_loss: 0.8467 - val_accuracy: 0.7770\n",
      "Epoch 11/20\n",
      "1251/1251 [==============================] - 0s 368us/sample - loss: 0.6179 - accuracy: 0.8034 - val_loss: 0.7852 - val_accuracy: 0.7986\n",
      "Epoch 12/20\n",
      "1251/1251 [==============================] - 0s 364us/sample - loss: 0.5946 - accuracy: 0.8114 - val_loss: 0.7369 - val_accuracy: 0.8058\n",
      "Epoch 13/20\n",
      "1251/1251 [==============================] - 0s 395us/sample - loss: 0.5239 - accuracy: 0.8425 - val_loss: 0.7011 - val_accuracy: 0.8058\n",
      "Epoch 14/20\n",
      "1251/1251 [==============================] - 0s 377us/sample - loss: 0.4786 - accuracy: 0.8593 - val_loss: 0.6697 - val_accuracy: 0.8129\n",
      "Epoch 15/20\n",
      "1251/1251 [==============================] - 0s 375us/sample - loss: 0.4529 - accuracy: 0.8657 - val_loss: 0.6516 - val_accuracy: 0.8201\n",
      "Epoch 16/20\n",
      "1251/1251 [==============================] - 0s 374us/sample - loss: 0.4198 - accuracy: 0.8753 - val_loss: 0.6345 - val_accuracy: 0.8201\n",
      "Epoch 17/20\n",
      "1251/1251 [==============================] - 0s 373us/sample - loss: 0.3777 - accuracy: 0.8953 - val_loss: 0.6171 - val_accuracy: 0.8201\n",
      "Epoch 18/20\n",
      "1251/1251 [==============================] - 0s 383us/sample - loss: 0.3341 - accuracy: 0.9049 - val_loss: 0.6001 - val_accuracy: 0.8201\n",
      "Epoch 19/20\n",
      "1251/1251 [==============================] - 0s 369us/sample - loss: 0.3087 - accuracy: 0.9161 - val_loss: 0.5858 - val_accuracy: 0.8417\n",
      "Epoch 20/20\n",
      "1251/1251 [==============================] - 0s 382us/sample - loss: 0.2975 - accuracy: 0.9185 - val_loss: 0.5710 - val_accuracy: 0.8345\n"
     ]
    }
   ],
   "source": [
    "# model.fit trains the model\n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "# You can see the validation loss decreasing slowly when you run this\n",
    "# Because val_loss is no longer decreasing we stop training to prevent overfitting\n",
    "class_weights = class_weight.compute_class_weight('balanced', list(np.unique(y_train_num)), y_train_num)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.5218549808880653\n",
      "Test accuracy: 0.4790717\n",
      "Counter({'society': 13055, 'junk': 10678, 'economy': 1234, 'sports': 889, 'other': 261, 'technology': 162, 'science': 26, 'entertainment': 23})\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predicted_labels = text_labels[np.argmax(predictions, axis=1)]\n",
    "predicted_labels.tofile(\"data/tf_test_labels\", sep=\"\\n\", format=\"%s\")\n",
    "print(Counter(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тренд зимы правильно красиво носить берет пережить зиму легко красиво теплые стильные расскажем правильно красиво носить французский берет комплекте самом деле берет самых универсальных головных уборов покажем подготовили полноценный гид шнуровке кож ...\n",
      "Actual label:junk\n",
      "Predicted label: junk\n",
      "\n",
      "канаде чёрную корову продали рекордные тысяч животное которому дали имя боролись фермеры канады мексики сша корову породы чёрный продали аукционе городе тысяч канадских долларов тысяч телеканал заплатить солидную сумму согласились генетикой даррен со ...\n",
      "Actual label:economy\n",
      "Predicted label: economy\n",
      "\n",
      "торговые павильоны ликвидируют возле речного вокзала хабаровске хабаровске ликвидируют точки шаурмой офисы туристических фирм располагались возле здания речного вокзала срок аренды объектов истек данным мэрии владельцы точек общепита протестовали про ...\n",
      "Actual label:other\n",
      "Predicted label: other\n",
      "\n",
      "суд обязал журналиста азара выплатить тыс хамовнический районный суд москвы обязал специального корреспондента новой газеты главного редактора журнала илью азара выплатить тысяч иску защите деловой репутации поданному семьёй проректора мгимо поданном ...\n",
      "Actual label:society\n",
      "Predicted label: society\n",
      "\n",
      "участники рок группы маневры запомнили яркой смелой тело нашли спустя месяцы смерти экс подмосковной группы маневры мать нашли мертвыми тела лежали ванной шее исполнительницы обнаружили веревку другой конец которой привязан нади журавель подворотен в ...\n",
      "Actual label:society\n",
      "Predicted label: junk\n",
      "\n",
      "цифровые автомобильные ключи станут умнее смартфоны позволят открыть авто некоторые современные автомобили открыть ключа функции способен выполнять смартфон такого решения недостатки примеру делать телефон вовсе сломался вскоре эта проблема решена ку ...\n",
      "Actual label:technology\n",
      "Predicted label: technology\n",
      "\n",
      "ученых избрали российскую академию наук андрей медведев любовь владимировна стали членами корреспондентами российской академии наук избрание новых членов академии общем собрании москве иркутские ученые андрей медведев любовь владимировна стали членам ...\n",
      "Actual label:science\n",
      "Predicted label: science\n",
      "\n",
      "законопроект регулировании электронной подписи принят первом чтении вводится институт доверенных третьих лиц повышаются требования деятельности удостоверяющих центров числе величине порога собственного капитала страховой ответственности минимальный р ...\n",
      "Actual label:society\n",
      "Predicted label: society\n",
      "\n",
      "погибший полынье омский школьник остался родительского присмотра второклассник остался присмотра погибший восьмилетний мальчик лед оказался родительского присмотра рассказали омском следкоме трагедия случилась центральном округе омска предварительным ...\n",
      "Actual label:society\n",
      "Predicted label: society\n",
      "\n",
      "многоэтажке сгорел лифт балакове пожар случился многоэтажном жилом доме загорелась шахта происшествие произошло проспекте героев огонь охватил пять квадратных метров сигнал бедствия поступил спасателям тушение выезжали семь пожарных три единицы балак ...\n",
      "Actual label:society\n",
      "Predicted label: society\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_data.text.iloc[i][:250], \"...\")\n",
    "    print('Actual label:' + test_data.category.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate -> calcul\n",
      "metrics -> metric\n",
      "for -> for\n",
      "each -> each\n",
      "label, -> label,\n",
      "and -> and\n",
      "find -> find\n",
      "their -> their\n",
      "average, -> average,\n",
      "weighted -> weight\n",
      "by -> by\n",
      "support -> support\n",
      "(the -> (the\n",
      "number -> number\n",
      "of -> of\n",
      "true -> true\n",
      "instances -> instanc\n",
      "for -> for\n",
      "each -> each\n",
      "label). -> label).\n"
     ]
    }
   ],
   "source": [
    "s = \"Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label).\".split(\" \")\n",
    "\n",
    "for w, ws in zip(s, news.stem(s, \"english\")):\n",
    "    print(f\"{w} -> {ws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
