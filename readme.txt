Мое решение для конкурса по кластеризации данных.
Для работы необходим Python >= 3.7 (входит в дистрибутив Debian 10.1.0). Никаких дополнительных зависимостей нет.

Как запускать:
  ./tgnews <этап> <папка с .html файлами> [<опции>]
Или
  python3 tgnews.py <этап> <папка с .html файлами> [<опции>]

Например:
  ./tgnews top /home/user/htmls_folders

Опции:
  --help - помощь
  --console - печатать тексты статей вместо JSON-формата вывода
  --similarity-cutoff <число> - изменить порог похожести для объединения статей в сюжеты. По умолчанию 0.25. Если задать
    большее число, то сюжеты будут разделяться на более узкие подтемы. Если задать меньшее, то новости будут
    сгруппированы в более крупные и общие сюжеты

Принципы работы:
Язык определяется по трем критериям:
  - модель из fastText основанная на словах
  - whatlang_rs (статистика по символам)
  - статистика по вебсайту-источнику новости

Отделение новостей от не новостей и категоризация делаются одновременно с помощью трехслойной нейронной сети, на вход
которой подается bag of words представление текста.

Объединение в сюжеты основано на статистике совпадений слов и пар слов между двумя текстами. Приближенный алгоритм
вычисляет самые похожие пары за линейное время. Внутри сюжета новости сортируются по времени. Заголовок сюжета -
заголовок самой свежей новости в нем.

Ранжирование сюжетов делается нейросетью, подобной той, что используется для классификации, которая предсказывает
количество статей в сюжете с данной новостью (популярность). Предсказанная популярность сюжета это средняя популярность
новостей в нем. Помимо популярности учитывается количество новостей в треде, средняя популярность источников новостей и
количество стран (вычисленное по источникам), опубликовавших сюжет.

Для сервера используется библиотека Sanic. База данных своя.

Исходники всех программ включены в архив в папку src. В их числе:
  src/tgnews - код самого приложения на питоне
  src/cpp_stuff - код на C++, выполняющий все вычислительно трудоемкие задачи. Включает в себя исходники используемых
      библиотек с открытым кодом в папке 3rdparty (все библиотеки с пермиссивными лицензиями)
  src/whatlang_cl - код на Rust, обертка для whatlang_cl
  src/model_building - Jupyter notebooks и сопутствующий код для тренировки включенных моделей (Tensorflow 2.0/Keras)

  data/ - все используемые модели
